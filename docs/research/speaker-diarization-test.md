# Speaker Diarization - Тестування та дослідження

## Мета
Визначити найкращий підхід для розпізнавання мовців та анотації транскрипції в конференційних записах.

## Тестові сценарії

### 1. Channel-Based Diarization (Simple)

**Тестовий сценарій:**
- Записати конференцію з двома потоками:
  - Потік 1: Мікрофон користувача
  - Потік 2: Системний аудіо (інші учасники)

**Очікуваний результат:**
- Кожен потік транскрибується окремо
- Результат об'єднується з labels "[Ви]" та "[Учасник]"
- Точність розпізнавання мовців: 100% (оскільки джерела різні)

**Код для тестування:**
```rust
// Псевдокод
let user_audio = record_microphone();
let remote_audio = record_system_audio();

let user_text = whisper.transcribe(&user_audio)?;
let remote_text = whisper.transcribe(&remote_audio)?;

let annotated = format!("[Ви] {}\n[Учасник] {}", user_text, remote_text);
```

**Метрики:**
- [ ] Чи правильно розпізнаються два потоки?
- [ ] Чи синхронізовані timestamps?
- [ ] Чи правильно об'єднуються результати?

### 2. pyannote-rs Integration Test

**Тестовий сценарій:**
- Записати конференцію з 3+ учасниками
- Використати pyannote-rs для diarization
- Транскрибувати через Whisper з анотацією

**Очікуваний результат:**
- pyannote визначає сегменти з різними мовцями
- Whisper транскрибує кожен сегмент
- Результат містить labels типу "[Speaker 1]", "[Speaker 2]", тощо

**Код для тестування:**
```rust
use pyannote_rs::DiarizationPipeline;

// 1. Завантажити модель (перший раз)
let pipeline = DiarizationPipeline::new()?;

// 2. Застосувати diarization
let diarization = pipeline.apply(&audio_samples)?;

// 3. Для кожного сегменту - транскрипція
for segment in diarization.segments {
    let segment_audio = extract_segment(&audio, segment.start, segment.end);
    let text = whisper.transcribe(&segment_audio, None)?;
    println!("[{}] {} ({:.2}s - {:.2}s)", 
        segment.speaker, text, segment.start, segment.end);
}
```

**Метрики:**
- [ ] Чи завантажуються моделі правильно?
- [ ] Чи правильно визначаються сегменти?
- [ ] Чи правильно ідентифікуються мовці?
- [ ] Чи синхронізовані з Whisper timestamps?
- [ ] Час обробки (очікується < 1 хвилина на годину аудіо)

### 3. Edge Cases Testing

#### 3.1 Одночасна мова
**Сценарій**: Два мовці говорять одночасно

**Очікуваний результат:**
- Channel-based: Кожен потік обробляється окремо (працює)
- pyannote-rs: Може плутати або пропускати один з мовців

#### 3.2 Тиша та паузи
**Сценарій**: Довгі паузи між репліками

**Очікуваний результат:**
- VAD правильно визначає сегменти з мовою
- Тиша не включається в транскрипцію

#### 3.3 Фоновий шум
**Сценарій**: Фоновий шум або музика

**Очікуваний результат:**
- VAD не класифікує шум як мову
- Транскрипція не містить шуму

#### 3.4 Один мовець (тільки мікрофон)
**Сценарій**: Користувач говорить сам (немає системного аудіо)

**Очікуваний результат:**
- Channel-based: Тільки "[Ви]" сегменти
- Не має бути "[Учасник]" сегментів

### 4. Performance Testing

**Метрики:**
- Час обробки 1 години аудіо:
  - [ ] Channel-based: очікується ~5-10 хвилин (Whisper транскрипція)
  - [ ] pyannote-rs: очікується < 1 хвилина (diarization) + Whisper time
- Використання пам'яті:
  - [ ] Channel-based: мінімальне (тільки Whisper)
  - [ ] pyannote-rs: +100-200MB для моделей
- CPU usage:
  - [ ] Channel-based: високе під час транскрипції
  - [ ] pyannote-rs: високе під час diarization + транскрипції

## Порівняльна таблиця

| Критерій | Channel-Based | pyannote-rs |
|----------|---------------|-------------|
| Складність реалізації | ⭐ Легко | ⭐⭐⭐ Складно |
| Точність (2 мовці) | ⭐⭐⭐⭐⭐ 100% | ⭐⭐⭐⭐ 95%+ |
| Точність (3+ мовці) | ⭐ Не працює | ⭐⭐⭐⭐ 90%+ |
| Швидкість | ⭐⭐⭐⭐ Швидко | ⭐⭐⭐ Середньо |
| Розмір моделей | ⭐⭐⭐⭐⭐ 0MB | ⭐⭐ 100-200MB |
| Залежності | ⭐⭐⭐⭐⭐ Мінімальні | ⭐⭐⭐ Додаткові |

## Рекомендації після тестування

### Якщо Channel-Based працює добре:
- Використовувати для MVP
- Додати pyannote-rs як опцію для advanced users

### Якщо потрібна підтримка 3+ мовців:
- Обов'язково інтегрувати pyannote-rs
- Можна зробити hybrid: channel-based за замовчуванням, pyannote-rs як опція

### Якщо pyannote-rs має проблеми:
- Почати з channel-based
- Дослідити альтернативи (energy-based VAD + clustering)

## Наступні кроки

1. Реалізувати channel-based diarization (Phase 1)
2. Протестувати на реальних конференційних записах
3. Оцінити, чи достатньо для більшості випадків
4. Якщо потрібно - інтегрувати pyannote-rs (Phase 2)

## Приклади очікуваного виводу

### Channel-Based (Simple)
```
[Ви] Привіт, як справи?
[Учасник] Добре, дякую. А у тебе?
[Ви] Теж добре. Почнемо конференцію?
[Учасник] Так, почнемо.
```

### pyannote-rs (Advanced)
```
[Speaker 1] Привіт, як справи?
[Speaker 2] Добре, дякую. А у тебе?
[Speaker 1] Теж добре. Почнемо конференцію?
[Speaker 2] Так, почнемо.
[Speaker 3] Я також готовий.
```
